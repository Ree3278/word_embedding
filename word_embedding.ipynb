{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e86fd",
   "metadata": {},
   "source": [
    "# Word Embedding Training\n",
    "\n",
    "Skip-Gram Vs Contineous Bag Of Words with self-Attention\n",
    "\n",
    "- Situation:\n",
    "    - Just learned Word Embedding, where I was taught there are two ways to generate meaninggul semantic word embeddings:    Skip-Gram and CBOW. I thought it was funny to try it out myself. So I decided to train embedding model using these two different methods, and give a rough comparsion to the result.\n",
    "\n",
    "- Task:\n",
    "    - Training two embedding models with Skip-Gram vs CBOW, and see which one may behave better by my own using experience.\n",
    "\n",
    "- Action:\n",
    "    - I pulled off an excerpt from a novel (titled \"The death of Hero\") as training corpus.\n",
    "    - Preprocessing the text, deleting unnecessary characters and symbols, spliting the text, forming a word2idx and idx2word\n",
    "\n",
    "        - Added \\<PAD\\>, \\<UNK\\> to handle unkown and padding too\n",
    "\n",
    "    - From here on, I took two routes:\n",
    "\n",
    "        - Route 1: training with Skip-Gram.\n",
    "            - Scan with a context window, form center, target pairs.   PS. center : a single word, target : a list of words appeared in the context of the center word. \n",
    "            - The key task is asking model to learn how to predict the context word from a given center word.\n",
    "            - The model I defined is the classic Word2vec Model, embedding layer + linear layer + softmax + crossentropyLoss\n",
    "\n",
    "        - Route 2: training with CBOW\n",
    "            - Scan with a context window again, form context, target pair. PS. context : list of words, target : a single word\n",
    "            - The task is training model to guess the missing value given a the surrounding words.\n",
    "            - The model I defined is embedding + self-attention + linear layer + softmax + crossentropyLoss\n",
    "- Result:\n",
    "    - The skip-gram approach had loss hard stuck at a very large number, not doing well\n",
    "    - The CBOW approach successfully converge to a small loss after 400 epoches, harry. Checked the embeding output, the result is interesting.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7065b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a328bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323149fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus:str = ''\n",
    "with open('corpus.txt', 'r') as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde6f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "text = corpus.lower()\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "corpus = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "corpus = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b604077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab \n",
    "vocab = set(corpus)\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = len(word2idx) # special tokens\n",
    "word2idx['<UNK>'] = len(word2idx) # special tokens\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08416ef3",
   "metadata": {},
   "source": [
    "## Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d648239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(899, [13, 1294]), (13, [899, 1294, 943]), (1294, [899, 13, 943, 329])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate context, and tokenize\n",
    "data = []\n",
    "window_size = 2\n",
    "for i, center in enumerate(corpus):\n",
    "    context = []\n",
    "    for j in range(max(0, i - window_size), min(len(corpus), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            context.append(word2idx[corpus[j]])  # [W]\n",
    "    data.append((word2idx[center], context))\n",
    "\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "649548e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to tensor\n",
    "c,context = zip(*data)\n",
    "center = torch.tensor(c, dtype=torch.long)\n",
    "\n",
    "targets = torch.zeros((len(center), vocab_size))\n",
    "for row in range(len(context)):\n",
    "    for col in context[row]: \n",
    "        targets[row, col] = 1\n",
    "\n",
    "targets[:5, :5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b72cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(center, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d34c7",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1 = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, center_words):\n",
    "        embeds = self.embeddings(center_words)  # [batch_size, embed_dim]\n",
    "        x = self.linear1(embeds)                # [batch_size, vocab_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb3d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (embeddings): Embedding(1365, 220)\n",
       "  (linear1): Linear(in_features=220, out_features=1365, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SkipGram(vocab_size, embed_dim=220)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e8f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3137.0091\n",
      "Epoch 10, Loss: 3130.8706\n",
      "Epoch 20, Loss: 3127.5402\n",
      "Epoch 30, Loss: 3122.9321\n",
      "Epoch 40, Loss: 3117.6036\n",
      "Epoch 50, Loss: 3113.7666\n",
      "Epoch 60, Loss: 3110.3580\n",
      "Epoch 70, Loss: 3105.8761\n",
      "Epoch 80, Loss: 3100.1104\n",
      "Epoch 90, Loss: 3098.2304\n",
      "Epoch 100, Loss: 3094.0000\n",
      "Epoch 110, Loss: 3089.6313\n",
      "Epoch 120, Loss: 3086.4785\n",
      "Epoch 130, Loss: 3083.3954\n",
      "Epoch 140, Loss: 3079.4711\n",
      "Epoch 150, Loss: 3074.7757\n",
      "Epoch 160, Loss: 3070.9609\n",
      "Epoch 170, Loss: 3070.0774\n",
      "Epoch 180, Loss: 3067.0077\n",
      "Epoch 190, Loss: 3061.8876\n",
      "Epoch 200, Loss: 3058.9116\n",
      "Epoch 210, Loss: 3056.8749\n",
      "Epoch 220, Loss: 3054.2147\n",
      "Epoch 230, Loss: 3051.5206\n",
      "Epoch 240, Loss: 3045.6635\n",
      "Epoch 250, Loss: 3043.7747\n",
      "Epoch 260, Loss: 3041.2251\n",
      "Epoch 270, Loss: 3039.6581\n",
      "Epoch 280, Loss: 3036.6365\n",
      "Epoch 290, Loss: 3033.3091\n",
      "Epoch 300, Loss: 3029.6859\n",
      "Epoch 310, Loss: 3028.3598\n",
      "Epoch 320, Loss: 3024.9406\n",
      "Epoch 330, Loss: 3021.1941\n",
      "Epoch 340, Loss: 3021.8115\n",
      "Epoch 350, Loss: 3017.9976\n",
      "Epoch 360, Loss: 3015.3558\n",
      "Epoch 370, Loss: 3014.7549\n",
      "Epoch 380, Loss: 3012.7236\n",
      "Epoch 390, Loss: 3008.6508\n",
      "Epoch 400, Loss: 3006.9785\n",
      "Epoch 410, Loss: 3005.6629\n",
      "Epoch 420, Loss: 3000.7314\n",
      "Epoch 430, Loss: 3000.1950\n",
      "Epoch 440, Loss: 2999.5795\n",
      "Epoch 450, Loss: 2996.0512\n",
      "Epoch 460, Loss: 2993.8523\n",
      "Epoch 470, Loss: 2993.0413\n",
      "Epoch 480, Loss: 2990.9893\n",
      "Epoch 490, Loss: 2989.2856\n",
      "Epoch 500, Loss: 2986.5344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "model.to(device)\n",
    "# training loop\n",
    "for epoch in range(501):\n",
    "    total_loss = 0\n",
    "    for center, context in dataloader:\n",
    "        center_batch = center.to(device)  # [B]\n",
    "        context_batch = context.to(device) # [B, V]\n",
    "\n",
    "        output = model(center_batch) # [B, V]\n",
    "        loss = loss_fn(output, context_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6a45c",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17634e71",
   "metadata": {},
   "source": [
    "### Form data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c86a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([814, 1003, 240, 1365, 1365, 1365], 180),\n",
       " ([180, 1003, 240, 211, 1365, 1365], 814),\n",
       " ([180, 814, 240, 211, 426, 1365], 1003),\n",
       " ([180, 814, 1003, 211, 426, 723], 240),\n",
       " ([814, 1003, 240, 426, 723, 114], 211)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get contexts and target pair\n",
    "\n",
    "data = []\n",
    "window_size = 3 # look 3 to the left and right\n",
    "for i, target in enumerate(corpus):\n",
    "    context = []\n",
    "    for j in range(max(0, i - window_size), min(len(corpus), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            context.append(word2idx[corpus[j]])\n",
    "\n",
    "    # pad for different lens\n",
    "    while len(context) < 6:\n",
    "        context.append(word2idx[\"<PAD>\"])\n",
    "    data.append((context, word2idx[target]))\n",
    "\n",
    "data[:5]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02641917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 814, 1003,  240, 1365, 1365, 1365],\n",
       "        [ 180, 1003,  240,  211, 1365, 1365],\n",
       "        [ 180,  814,  240,  211,  426, 1365],\n",
       "        ...,\n",
       "        [1003,  522,  835,  994,  174, 1365],\n",
       "        [ 522,  835, 1003,  174, 1365, 1365],\n",
       "        [ 835, 1003,  994, 1365, 1365, 1365]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to tensor\n",
    "\n",
    "context_list, target_list = zip(*data)\n",
    "\n",
    "target_tensor = torch.tensor(target_list)\n",
    "\n",
    "context_tensor = torch.tensor(context_list, dtype=torch.long) # [T, 6]\n",
    "\n",
    "context_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8cbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(context_tensor,target_tensor)\n",
    "dataloader = DataLoader(ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db583189",
   "metadata": {},
   "source": [
    "### Define Self-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "322b7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key   = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.scale = d_model**0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn_scores = Q @ K.transpose(-2, -1) / self.scale\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        return attn_weights @ V # [S, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a8830",
   "metadata": {},
   "source": [
    "### Define CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0339e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.self_attention = SimpleSelfAttention(embed_dim)\n",
    "        nodes = embed_dim * seq_len // 3 if embed_dim * seq_len > 3 else 3\n",
    "        self.linear1 = nn.Linear(embed_dim * seq_len, nodes)\n",
    "        self.linear2 = nn.Linear(nodes, vocab_size)\n",
    "    \n",
    "    def forward(self, x): # [S]\n",
    "        embed = self.embedding(x)\n",
    "        x = self.self_attention(embed).view((1, -1))\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x # [V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2569d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, seq_len=window_size*2, embed_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001d0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 29553.0728\n",
      "Epoch 10, Loss: 24741.6069\n",
      "Epoch 20, Loss: 23634.5078\n",
      "Epoch 30, Loss: 22257.3190\n",
      "Epoch 40, Loss: 20532.0775\n",
      "Epoch 50, Loss: 18555.5391\n",
      "Epoch 60, Loss: 16459.4669\n",
      "Epoch 70, Loss: 14386.7752\n",
      "Epoch 80, Loss: 12418.4807\n",
      "Epoch 90, Loss: 10614.9805\n",
      "Epoch 100, Loss: 8963.7750\n",
      "Epoch 110, Loss: 7499.7051\n",
      "Epoch 120, Loss: 6238.4146\n",
      "Epoch 130, Loss: 5160.8037\n",
      "Epoch 140, Loss: 4272.6087\n",
      "Epoch 150, Loss: 3508.1090\n",
      "Epoch 160, Loss: 2875.7679\n",
      "Epoch 170, Loss: 2363.8509\n",
      "Epoch 180, Loss: 1903.0090\n",
      "Epoch 190, Loss: 1527.5460\n",
      "Epoch 200, Loss: 1210.3182\n",
      "Epoch 210, Loss: 960.7232\n",
      "Epoch 220, Loss: 757.4371\n",
      "Epoch 230, Loss: 580.9135\n",
      "Epoch 240, Loss: 447.8394\n",
      "Epoch 250, Loss: 337.9303\n",
      "Epoch 260, Loss: 254.7353\n",
      "Epoch 270, Loss: 194.7736\n",
      "Epoch 280, Loss: 139.9516\n",
      "Epoch 290, Loss: 112.7440\n",
      "Epoch 300, Loss: 88.0148\n",
      "Epoch 310, Loss: 68.0574\n",
      "Epoch 320, Loss: 53.6810\n",
      "Epoch 330, Loss: 43.5100\n",
      "Epoch 340, Loss: 34.0334\n",
      "Epoch 350, Loss: 27.5803\n",
      "Epoch 360, Loss: 22.5688\n",
      "Epoch 370, Loss: 16.4490\n",
      "Epoch 380, Loss: 13.6680\n",
      "Epoch 390, Loss: 10.8771\n",
      "Epoch 400, Loss: 9.3042\n",
      "Epoch 410, Loss: 7.7031\n",
      "Epoch 420, Loss: 6.8241\n",
      "Epoch 430, Loss: 6.3215\n",
      "Epoch 440, Loss: 5.7432\n",
      "Epoch 450, Loss: 5.3701\n",
      "Epoch 460, Loss: 5.0527\n",
      "Epoch 470, Loss: 4.7800\n",
      "Epoch 480, Loss: 4.5374\n",
      "Epoch 490, Loss: 4.3235\n",
      "Epoch 500, Loss: 4.1274\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# training loop\n",
    "for epoch in range(501):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        target_ = target.to(device)  # [1]\n",
    "        context_ = context.to(device) # [S]\n",
    "\n",
    "        output = model(context_) # [V]\n",
    "        loss = loss_fn(output, target_)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dddc98dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight grad norm: 0.016792\n",
      "self_attention.query.weight grad norm: 0.056579\n",
      "self_attention.query.bias grad norm: 0.005814\n",
      "self_attention.key.weight grad norm: 0.053965\n",
      "self_attention.key.bias grad norm: 0.000000\n",
      "self_attention.value.weight grad norm: 0.056995\n",
      "self_attention.value.bias grad norm: 0.012227\n",
      "linear1.weight grad norm: 0.131273\n",
      "linear1.bias grad norm: 0.004745\n",
      "linear2.weight grad norm: 0.198066\n",
      "linear2.bias grad norm: 0.002480\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.data.norm(2).item()\n",
    "        print(f\"{name} grad norm: {grad_norm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "724aed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'say': tensor([-0.6516, -0.5228, -2.6756,  1.0574, -1.0365, -0.9951, -0.4743,  0.6886,\n",
      "        -0.0177,  1.1187, -0.5014,  0.1908, -1.3898, -1.5563,  1.4845,  1.0136,\n",
      "        -1.0156, -0.5403,  0.5217,  1.2004, -0.4451,  0.1688,  2.5683, -0.2973,\n",
      "        -0.2957,  0.3516, -0.1253,  1.1774, -0.8941, -0.2151, -0.9814,  1.4599,\n",
      "         1.0003, -0.6810,  1.0453,  1.0736, -1.6421, -0.7032, -0.8412,  0.6861,\n",
      "         0.7410,  0.3247,  0.2107,  0.4246,  0.8145, -0.4981,  0.4476, -2.1404,\n",
      "        -2.7737,  1.0773,  0.4306,  1.7925,  0.8574, -0.7026,  0.6058,  1.9203,\n",
      "        -0.5291, -0.8033, -0.9161, -0.8780,  0.3470,  0.5998,  0.9965,  0.5328,\n",
      "        -0.2157, -0.2082,  0.9472,  1.0053, -1.0803, -0.8169,  2.5357,  0.5682,\n",
      "        -1.7280,  1.4474, -1.3752, -2.2571,  1.9295, -0.5921, -0.3369,  1.0598,\n",
      "         0.0266,  0.6273,  2.0813, -0.7890, -0.6126,  0.4479,  0.0099, -0.5917,\n",
      "        -1.1267,  0.2658,  1.4372, -0.8784,  2.9819,  0.7703, -1.3374, -0.2320,\n",
      "         0.1725, -1.2790, -0.6814, -0.4513], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.embedding.weight.data\n",
    "print(f\"Embedding for 'say': {embeddings[word2idx['the']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "203f6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaguely\n",
      "possibly\n",
      "damn\n",
      "noble\n",
      "other\n",
      "corpse\n",
      "chemise\n",
      "avoiding\n",
      "inadequate\n",
      "unless\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "new_model = model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    center_vecs = new_model.embedding.weight\n",
    "    sim = F.cosine_similarity(center_vecs[word2idx[\"vaguely\"]].unsqueeze(0), center_vecs)\n",
    "    topk = torch.topk(sim, k=10)\n",
    "    for i in topk.indices:\n",
    "        print(idx2word[i.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfd134c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictatorship\n",
      "peoples\n",
      "matter\n",
      "except\n",
      "love\n",
      "toils\n",
      "light\n",
      "out\n",
      "more\n",
      "parts\n"
     ]
    }
   ],
   "source": [
    "test_words = '''even in a of the intelligentsia'''.split()\n",
    "\n",
    "vec = [word2idx[t] for t in test_words]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    word_vecs = torch.tensor(vec, dtype=torch.long)\n",
    "    #print(word_vecs.shape)\n",
    "\n",
    "    result = F.softmax(new_model(word_vecs), dim = -1).squeeze()\n",
    "    topk = torch.topk(result, k = 10)\n",
    "    for i in topk.indices:\n",
    "        print(idx2word[i.item()])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
